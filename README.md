# âš¡ Realtime Data Streaming | End-to-End Data Engineering Project

This project demonstrates a **complete realtime data engineering pipeline** â€” from data ingestion to processing, storage, and visualization. It integrates the full modern data stack with tools like **Apache Kafka, Airflow, Spark, Cassandra**, and **Docker**, simulating a production-grade environment for streaming analytics.

---

## ğŸ§  **System Architecture**


c:\Users\mhmda\OneDrive\Documents\My_Programming_Projects\Realtime Data Streaming, End-to-End Data Engineering Project\e2e-data-engineering-main\Data engineering architecture.png


The system is designed as an interconnected, containerized data pipeline:

1. **Data Source** â€” Generates live data using the `randomuser.me` API  
2. **Apache Airflow** â€” Orchestrates the pipeline, scheduling and fetching data into PostgreSQL  
3. **Apache Kafka + Zookeeper** â€” Streams data from PostgreSQL to Spark in realtime  
4. **Schema Registry & Control Center** â€” Ensures stream management and monitoring  
5. **Apache Spark** â€” Processes incoming Kafka streams for transformation and enrichment  
6. **Cassandra** â€” Stores processed data for fast and scalable retrieval  
7. **Docker** â€” Containerizes the full pipeline for easy deployment and scalability  

---

## ğŸ§© **Technologies Used**

| Layer | Tools & Frameworks |
|-------|--------------------|
| **Orchestration** | Apache Airflow |
| **Messaging / Streaming** | Apache Kafka, Apache Zookeeper |
| **Processing** | Apache Spark |
| **Storage** | PostgreSQL, Cassandra |
| **Containerization** | Docker, Docker Compose |
| **Language** | Python |

---

## ğŸ¯ **What Youâ€™ll Learn**

- Building a realtime data pipeline from ingestion to storage  
- Orchestrating workflows with Apache Airflow  
- Implementing event-driven architecture with Apache Kafka  
- Distributed synchronization with Apache Zookeeper  
- Data transformation and enrichment using Apache Spark  
- Persisting processed data in Cassandra and PostgreSQL  
- Containerizing and scaling with Docker Compose  

---

## âš™ï¸ **Getting Started**

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/MohamadAub/realtime-data-streaming-pipeline.git
cd realtime-data-streaming-pipeline

2ï¸âƒ£ Start the Docker Services
docker-compose up

3ï¸âƒ£ Access Airflow UI

Open your browser and go to:
http://localhost:8080

Use Airflow to trigger the pipeline DAGs.

4ï¸âƒ£ Check Kafka Streams

Control Center URL:
http://localhost:9021

5ï¸âƒ£ Explore Cassandra & PostgreSQL

1- Cassandra: stores processed data
2- PostgreSQL: raw data from Airflow ingestion


Project Architecture Diagram

      +---------------------+
      |  RandomUser API     |
      +----------+----------+
                 |
                 v
        +--------+---------+
        |   Apache Airflow  |----> PostgreSQL
        +--------+---------+
                 |
                 v
        +--------+--------+
        |   Apache Kafka  |
        +--------+--------+
                 |
                 v
        +--------+--------+
        |  Apache Spark   |
        +--------+--------+
                 |
                 v
        +--------+--------+
        |   Cassandra     |
        +-----------------+

ğŸ“š Learning Goals
	â€¢	Hands-on understanding of data streaming concepts
	â€¢	Event-driven architecture design using Kafka
	â€¢	Containerized deployments with Docker
	â€¢	Batch vs. streaming data processing
	â€¢	Workflow automation using Airflow

â¸»

ğŸš€ Future Improvements
	â€¢	Add data visualization dashboards (Grafana / Power BI)
	â€¢	Include data quality checks (Great Expectations)
	â€¢	Extend to cloud services (AWS MSK, GCP Pub/Sub, or Azure Event Hubs)
	â€¢	Real-time alerting & monitoring system

â¸»

ğŸ¤ Contributing

Contributions are welcome!
Fork the repository, make your changes, and submit a Pull Request.

â¸»

ğŸ“„ License

This project is licensed under the MIT License.

â¸»

ğŸŒ Connect with Me

ğŸ‘¤ Mohamad El Ayoubi
ğŸ”— https://github.com/MohamadAub
âœ‰ mmelayoubi@gmail.com
ğŸ“ Paris, France